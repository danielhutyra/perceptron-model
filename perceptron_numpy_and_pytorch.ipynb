{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "h_09hBlWjxec"
   },
   "source": [
    "# Perceptron\n",
    "\n",
    "### Zadanie 1\n",
    "Napisz kod klasy percpetronu wykorzystując jedynie bibliotekę numpy:\n",
    "\n",
    "* przypisz początkowe wagi $w$ oraz odchylenie peceptronu losowo, rozkład normalny z przedziału (0,1). Zakładamy, że rozmiar wejścia nie jest określony z góry - może być podana przez użytkownika lub odczytana bezpośrednio z danych.\n",
    "\n",
    "* Napisz funkcję forward, która mając podane wejście $x$ zwraca wyjście perceptronu. Przyjmij sigmoidalną funkcję aktywacji.\n",
    "$net=w^Tx$\n",
    "  $y=\\sigma(net)$\n",
    "\n",
    "\n",
    "* Napisz pętlę/funkcję, która uczy perceptron na całym zbiorze treningowym zgodnie z metodą największego spadku gradientu . Funkcja ta powinna dla każdego przykładu ze zbioru treningowego:\n",
    "  * obliczyć wyjście $y$ \n",
    "  * obliczyć funkcję błędu(loss) $L=\\frac{1}{2}\\sum_{i=1}^{N}(d-y)^2$, $d$ -porządane wyjście, N liczba obserwacji\n",
    "  * obliczyć zmianę wagi $\\Delta w=-n\\frac{\\partial L}{\\partial w}=-n\\frac{\\partial L}{\\partial y}\\frac{\\partial y}{\\partial w}=-n\\frac{\\partial L}{\\partial y}\\frac{\\partial y}{\\partial net}\\frac{\\partial net}{\\partial w}$, czyli dla wybranej funkcji aktywacji $\\frac{\\partial y}{\\partial net}=y(1-y)$, a zatem $\\Delta w=n(d-y)y(1-y)*x$ ,gdzie n- współczynnik uczenia(learning rate),\n",
    "  * zauktualizować wagi $w'=w+\\Delta w$\n",
    "\n",
    "Pod koniec każdej epoki wyświetl błąd oraz liczbę poprawnie zaklasyfikowanych obserwacji ze zbioru testowego.\n",
    "\n",
    "Trenowanie perceptronu kończy się gdy została wykonana określona liczba epok.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XE0TsuRnjnna"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "X_train=np.array([[10,11], [20,30], [5,4], [8,20], [22, 10], [9, 10], [5, 17], [5, 50], [50,5], [3,4], [6,7], [-10,10], [-10,1]])\n",
    "y_train=np.array([0,1, 0, 1, 0, 0, 1,1,0,0,0, 1,1])\n",
    "assert len(X_train)==len(y_train)\n",
    "\n",
    "\n",
    "X_test=np.array([[0,10], [20,30], [-20, 1], [19,20], [10,5], [2,1]])\n",
    "y_test=np.array([1,1,1,0,0,0])\n",
    "assert len(X_test)==len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "frBIcZYyPPaW",
    "outputId": "5005e7d4-1ba4-4c77-deaa-72d185bdbec0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20 30]\n"
     ]
    }
   ],
   "source": [
    "from functools import reduce\n",
    "lols = [1,2,3, 9\n",
    "        ]\n",
    "print(X_train[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "skncmEr-QG0R"
   },
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "\n",
    "class Perceptron(object):\n",
    "  def __init__(self, no_of_inputs, epochs = 10, learning_rate = 0.01):\n",
    "    self.no_of_inputs = no_of_inputs\n",
    "    self.epochs = epochs\n",
    "    self.learning_rate = learning_rate\n",
    "    self.w = np.random.rand(no_of_inputs)\n",
    "    self.b = np.random.rand(1)\n",
    "\n",
    "  def forward(self, inputs):\n",
    "    net = np.dot(inputs, self.w) + self.b\n",
    "    if net.size == 1:\n",
    "      if net > 0:\n",
    "        activation = 1\n",
    "      else:\n",
    "        activation = 0\n",
    "    else:\n",
    "      activation = []\n",
    "      for x in np.nditer(net):\n",
    "        if x > 0:\n",
    "          activation.append(1)\n",
    "        else:\n",
    "          activation.append(0)\n",
    "    return activation\n",
    "\n",
    "  def train(self, train_inputs, labels):\n",
    "    for _ in range(self.epochs):\n",
    "      losses = []\n",
    "      correct_predictions = 0\n",
    "      for inputs, label in zip(train_inputs, labels):\n",
    "        prediction = self.forward(inputs)\n",
    "        self.w  += self.learning_rate * (label - prediction) * inputs\n",
    "        self.b  += self.learning_rate * (label - prediction)\n",
    "        losses.append((label - prediction)**2)\n",
    "        if prediction == label:\n",
    "          correct_predictions += 1\n",
    "      loss = (1/(self.no_of_inputs))*(reduce(lambda x, y: x+y, losses))\n",
    "      print(\"Loss is {} and there were {} correct predictions \".format(loss, correct_predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 384
    },
    "colab_type": "code",
    "id": "n5iiIr_fZi0Z",
    "outputId": "c7856a48-3204-4531-ccfa-36708831c9f1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss is 3.0 and there were 7 correct predictions \n",
      "Loss is 0.5 and there were 12 correct predictions \n",
      "Loss is 2.0 and there were 9 correct predictions \n",
      "Loss is 0.0 and there were 13 correct predictions \n",
      "Loss is 0.0 and there were 13 correct predictions \n",
      "Loss is 0.0 and there were 13 correct predictions \n",
      "Loss is 0.0 and there were 13 correct predictions \n",
      "Loss is 0.0 and there were 13 correct predictions \n",
      "Loss is 0.0 and there were 13 correct predictions \n",
      "Loss is 0.0 and there were 13 correct predictions \n",
      "Loss is 0.0 and there were 13 correct predictions \n",
      "Loss is 0.0 and there were 13 correct predictions \n",
      "Loss is 0.0 and there were 13 correct predictions \n",
      "Loss is 0.0 and there were 13 correct predictions \n",
      "Loss is 0.0 and there were 13 correct predictions \n",
      "Loss is 0.0 and there were 13 correct predictions \n",
      "Loss is 0.0 and there were 13 correct predictions \n",
      "Loss is 0.0 and there were 13 correct predictions \n",
      "Loss is 0.0 and there were 13 correct predictions \n",
      "Loss is 0.0 and there were 13 correct predictions \n"
     ]
    }
   ],
   "source": [
    "model = Perceptron(2, epochs = 20)\n",
    "model.train(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nJZxrGf9xHXC"
   },
   "outputs": [],
   "source": [
    "numpy_model_results = model.forward(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SNhb3cie-a5w"
   },
   "source": [
    "### Zadanie2 \n",
    "Modyfikując model z poprzednich zajęć, utwórz podobny jak w zad1 perceprtron w PyTorch. \n",
    "Znajdź odpowiednią fukncję aktywacji w Pytorch(https://pytorch.org/docs/stable/nn.html#) i wykorzystaj ją do przkształcenia sygnału liniowego (nn.Linear)\n",
    "Porównaj wyniki z perceptronem z zadania 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eI-gM4vha-YO"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class PerceptronT(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.linear = nn.Linear(input_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear(x)\n",
    "        out = self.sigmoid(x) #obliczanie wyjścia\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 384
    },
    "colab_type": "code",
    "id": "4EQhp3cTcJ2q",
    "outputId": "05c1ffd2-6d9e-4812-ae51-af20e7ee9c7f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss 0.41703203320503235\n",
      "epoch 2, loss 0.38103076815605164\n",
      "epoch 3, loss 0.3489570915699005\n",
      "epoch 4, loss 0.3278135657310486\n",
      "epoch 5, loss 0.3103237450122833\n",
      "epoch 6, loss 0.2927044630050659\n",
      "epoch 7, loss 0.2736669182777405\n",
      "epoch 8, loss 0.2531040608882904\n",
      "epoch 9, loss 0.23173223435878754\n",
      "epoch 10, loss 0.21066807210445404\n",
      "epoch 11, loss 0.19084151089191437\n",
      "epoch 12, loss 0.17266035079956055\n",
      "epoch 13, loss 0.15611225366592407\n",
      "epoch 14, loss 0.14103099703788757\n",
      "epoch 15, loss 0.12727323174476624\n",
      "epoch 16, loss 0.11477208882570267\n",
      "epoch 17, loss 0.10352408140897751\n",
      "epoch 18, loss 0.0935564860701561\n",
      "epoch 19, loss 0.0848981961607933\n",
      "epoch 20, loss 0.07756074517965317\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.MSELoss()\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "#ustalenie wymiarów danych wejściowych oraz wyjścia\n",
    "input_dim = 2 # rozmiar wejścia\n",
    "output_dim = 1 # rozmiar wyjścia -  będzie to 1 bo chcemy otrzymać jedną wartość liczbową\n",
    "\n",
    "#utworzenie obiektu modelu regrsji liniowej\n",
    "model = PerceptronT(input_dim, output_dim)\n",
    "\n",
    "\n",
    "#Ustalenie metody optymalizacji, tutaj SGD, oraz jej parametrów\n",
    "\n",
    "learning_rate = 0.01\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "#trenowanie modelu\n",
    "\n",
    "epochs = 20\n",
    "for epoch in range(epochs):\n",
    "    epoch += 1\n",
    "    # konwersja tablicy numpy na tensory o odpowiednich wymiarach\n",
    "    inputs = torch.from_numpy(X_train.astype(np.float32))#.unsqueeze(1)\n",
    "    #print(inputs)\n",
    "    labels = torch.from_numpy(y_train.astype(np.float32))\n",
    "\n",
    "    # Czyszczenie gradientów\n",
    "    optimizer.zero_grad() \n",
    "\n",
    "    # oblicznie wyjścia (w grafie obliczeń: od początku do końca)\n",
    "    outputs = model(inputs)\n",
    "\n",
    "    # Obliczanie błedu\n",
    "    loss = criterion(outputs, labels.unsqueeze(1))\n",
    "\n",
    "    # Obliczanie gradientów dla parametrów (od końca do początku)\n",
    "    loss.backward()\n",
    "\n",
    "    # Aktualizacja parametrów\n",
    "    optimizer.step()\n",
    "    \n",
    "    print('epoch {}, loss {}'.format(epoch, loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OAvyDCJTuipO"
   },
   "outputs": [],
   "source": [
    "pytorch_model_results = model(torch.from_numpy(X_test.astype(np.float32)))\n",
    "pytorch_model_results = np.where(pytorch_model_results > 0.5, 1, 0).reshape(1,6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 127
    },
    "colab_type": "code",
    "id": "i-n8GFewz97V",
    "outputId": "c82b6602-9c7e-47ee-81cb-1833df808077"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numpy model results:\n",
      "[1, 1, 1, 0, 0, 0]\n",
      "Pytorch model results:\n",
      "[[1 1 1 0 0 0]]\n",
      "Real values:\n",
      "[1 1 1 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "print(\"Numpy model results:\")\n",
    "print(numpy_model_results)\n",
    "print(\"Pytorch model results:\")\n",
    "print(pytorch_model_results)\n",
    "print(\"Real values:\")\n",
    "print(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RRfo9oXY1L28"
   },
   "source": [
    "Oba modele wszystko trafiły! Sukces!"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "WUM2z.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
